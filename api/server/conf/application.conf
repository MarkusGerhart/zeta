# This is the main configuration file for the application.
# ~~~~~

# application will be served behind a load balancer
#play.http.context="/api/"

# MongoDB Configration
# ~~~~~
#
mongodb.ip="mongodb"
mongodb.port=27017
mongodb.name="modigen_v3"
mongodb.uri = "mongodb://mongodb:27017/modigen_v3"

play.modules.enabled += "play.modules.reactivemongo.ReactiveMongoModule"

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
play.crypto.secret="changeme"
play.crypto.secret=${?APPLICATION_SECRET}

# The application languages
# ~~~~~
play.i18n.langs=["en"]

# Registers the request handler
# ~~~~~
play.http.requestHandler = "play.api.http.DefaultHttpRequestHandler"

# Registers the filters
# ~~~~~
play.http.filters = "utils.Filters"

# The application DI modules
# ~~~~~
play.modules.enabled += "modules.BaseModule"
play.modules.enabled += "modules.JobModule"
play.modules.enabled += "modules.SilhouetteModule"
play.modules.enabled += "play.api.libs.mailer.MailerModule"

play.modules.disabled += "com.mohiva.play.silhouette.api.actions.SecuredErrorHandlerModule"
play.modules.disabled += "com.mohiva.play.silhouette.api.actions.UnsecuredErrorHandlerModule"

# Akka config
#akka {
 # actor.provider = "akka.cluster.ClusterActorRefProvider"
#
 # remote.netty.tcp.port=0

  #loglevel = "INFO"
  #jvm-exit-on-fatal-error=off

  # Auth token cleaner
  #quartz.schedules.AuthTokenCleaner {
  #  expression = "0 0 */1 * * ?"
  #  timezone = "UTC"
  #  description = "cleanup the auth tokens on every hour"
  #}
#}

# Play mailer
play.mailer {
  host = smtp.gmail.com
  from = "iodfjvio@gmail.com"
  ssl = true
  port = 465
  user = "iodfjvio@gmail.com"
  password = jVcSwooo
  debug = true
  #mock = true
}

# Security Filter Configuration - Content Security Policy
play.filters.headers {
  contentSecurityPolicy = "default-src 'self';"
  contentSecurityPolicy = ${play.filters.headers.contentSecurityPolicy}" img-src 'self' data: *.fbcdn.net *.twimg.com *.googleusercontent.com *.xingassets.com vk.com *.yimg.com secure.gravatar.com;"
  contentSecurityPolicy = ${play.filters.headers.contentSecurityPolicy}" style-src 'self' 'unsafe-inline' cdnjs.cloudflare.com maxcdn.bootstrapcdn.com cdn.jsdelivr.net fonts.googleapis.com;"
  contentSecurityPolicy = ${play.filters.headers.contentSecurityPolicy}" font-src 'self' fonts.gstatic.com fonts.googleapis.com cdnjs.cloudflare.com;"
  contentSecurityPolicy = ${play.filters.headers.contentSecurityPolicy}" script-src 'self' 'unsafe-eval' 'unsafe-inline' clef.io cdnjs.cloudflare.com;"
  contentSecurityPolicy = ${play.filters.headers.contentSecurityPolicy}" connect-src 'self' ws: twitter.com *.xing.com;"
  contentSecurityPolicy = ${play.filters.headers.contentSecurityPolicy}" frame-src clef.io;"
}

# CSRF Configuration
play.filters.csrf.contentType.whiteList=[
  "application/json",
  "text/html"
]



# to discover our seed nodes we require a costum application loader which setup the actor configuration with the seed node
play.application.loader = "CustomApplicationLoader"

#play.akka.config = "akka"

play.akka.actor-system = "ClusterSystem"

akka {

  # Auth token cleaner
  quartz.schedules.AuthTokenCleaner {
    expression = "0 0 */1 * * ?"
    timezone = "UTC"
    description = "cleanup the auth tokens on every hour"
  }

  actor.provider = "akka.cluster.ClusterActorRefProvider"

  remote.netty.tcp.port=0
  # remote.netty.tcp.hostname=127.0.0.1

  cluster {
    metrics.enabled=off
    seed-nodes = []
    # "akka.tcp://ClusterSystem@127.0.0.1:2551"
    #min-nr-of-members = 2
    auto-down-unreachable-after = 10s
  }

  actor {
    kryo  {
      # Possibles values for type are: graph or nograph
      # graph supports serialization of object graphs with shared nodes
      # and cyclic references, but this comes at the expense of a small overhead
      # nograph does not support object grpahs with shared nodes, but is usually faster
      type = "graph"


      # Possible values for idstrategy are:
      # default, explicit, incremental
      #
      # default - slowest and produces bigger serialized representation. Contains fully-
      # qualified class names (FQCNs) for each class
      #
      # explicit - fast and produces compact serialized representation. Requires that all
      # classes that will be serialized are pre-registered using the "mappings" and "classes"
      # sections. To guarantee that both sender and receiver use the same numeric ids for the same
      # classes it is advised to provide exactly the same entries in the "mappings" section
      #
      # incremental - fast and produces compact serialized representation. Support optional
      # pre-registering of classes using the "mappings" and "classes" sections. If class is
      # not pre-registered, it will be registered dynamically by picking a next available id
      # To guarantee that both sender and receiver use the same numeric ids for the same
      # classes it is advised to pre-register them using at least the "classes" section

      idstrategy = "default"

      # Define a default size for byte buffers used during serialization
      buffer-size = 4096

      # The serialization byte buffers are doubled as needed until they exceed maxBufferSize and an exception is thrown. Can be -1 for no maximum.
      max-buffer-size = -1

      # Define a default size for serializer pool
      serializer-pool-size = 16

      # If set, akka uses manifests to put a class name
      # of the top-level object into each message
      use-manifests = false

      # Enable transparent compression of serialized messages
      # accepted values are: off | lz4 | deflate
      compression = off

      # Log implicitly registered classes. Useful, if you want to know all classes
      # which are serialized
      implicit-registration-logging = false

      # If enabled, Kryo logs a lot of information about serialization process.
      # Useful for debugging and lowl-level tweaking
      kryo-trace = false

      # If enabled, Kryo uses internally a map detecting shared nodes.
      # This is a preferred mode for big object graphs with a lot of nodes.
      # For small object graphs (e.g. below 10 nodes) set it to false for
      # better performance.
      kryo-reference-map = true

      # Define mappings from a fully qualified class name to a numeric id.
      # Smaller ids lead to smaller sizes of serialized representations.
      #
      # This section is mandatory for idstartegy=explciit
      # This section is optional  for idstartegy=incremental
      # This section is ignored   for idstartegy=default
      #
      # The smallest possible id should start at 20 (or even higher), because
      # ids below it are used by Kryo internally e.g. for built-in Java and
      # Scala types
      mappings {
        # fully.qualified.classname1 = id1
        # fully.qualified.classname2 = id2
      }

      # Define a set of fully qualified class names for
      # classes to be used for serialization.
      # The ids for those classes will be assigned automatically,
      # but respecting the order of declaration in this section
      #
      # This section is optional  for idstartegy=incremental
      # This section is ignored   for idstartegy=default
      # This section is optional  for idstartegy=explicit
      classes = [
        # fully.qualified.classname2
      ]
    }

    serializers {
      java = "akka.serialization.JavaSerializer"
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "java.io.Serializable" = kryo
    }
  }

  extensions = [
    "akka.cluster.client.ClusterClientReceptionist",
    "akka.cluster.metrics.ClusterMetricsExtension",
    "akka.cluster.pubsub.DistributedPubSub",
    "com.romix.akka.serialization.kryo.KryoSerializationExtension$"]

  persistence {
    journal.plugin = "akka.persistence.journal.leveldb-shared"
    journal.leveldb-shared.store {
      # DO NOT USE 'native = off' IN PRODUCTION !!!
      native = off
      dir = "target/shared-journal"
    }
    snapshot-store.plugin = "akka.persistence.snapshot-store.local"
    snapshot-store.local.dir = "target/snapshots"
  }

  cluster.failure-detector {
    threshold = 12
    acceptable-heartbeat-pause = 60s
    heartbeat-interval = 5s
    heartbeat-request {
      expected-response-after = 20s
    }
  }

  # Settings for the ClusterShardingExtension
  cluster.sharding {

    # The extension creates a top level actor with this name in top level system scope,
    # e.g. '/system/sharding'
    #guardian-name = sharding

    # Specifies that entities runs on cluster nodes with a specific role.
    # If the role is not specified (or empty) all nodes in the cluster are used.
    role = "developer"

    # When this is set to 'on' the active entity actors will automatically be restarted
    # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
    # due to rebalance or crash.
    remember-entities = on

    # If the coordinator can't store state changes it will be stopped
    # and started again after this duration, with an exponential back-off
    # of up to 5 times this duration.
    coordinator-failure-backoff = 5 s

    # The ShardRegion retries registration and shard location requests to the
    # ShardCoordinator with this interval if it does not reply.
    retry-interval = 2 s

    # Maximum number of messages that are buffered by a ShardRegion actor.
    buffer-size = 100000

    # Timeout of the shard rebalancing process.
    handoff-timeout = 60 s

    # Time given to a region to acknowledge it's hosting a shard.
    shard-start-timeout = 10 s

    # If the shard is remembering entities and can't store state changes
    # will be stopped and then started again after this duration. Any messages
    # sent to an affected entity may be lost in this process.
    shard-failure-backoff = 10 s

    # If the shard is remembering entities and an entity stops itself without
    # using passivate. The entity will be restarted after this duration or when
    # the next message for it is received, which ever occurs first.
    entity-restart-backoff = 10 s

    # Rebalance check is performed periodically with this interval.
    rebalance-interval = 10 s

    # Absolute path to the journal plugin configuration entity that is to be
    # used for the internal persistence of ClusterSharding. If not defined
    # the default journal plugin is used. Note that this is not related to
    # persistence used by the entity actors.
    #journal-plugin-id = ""

    # Absolute path to the snapshot plugin configuration entity that is to be
    # used for the internal persistence of ClusterSharding. If not defined
    # the default snapshot plugin is used. Note that this is not related to
    # persistence used by the entity actors.
    #snapshot-plugin-id = ""

    # Parameter which determines how the coordinator will be store a state
    # valid values either "persistence" or "ddata"
    # The "ddata" mode is experimental, since it depends on the experimental
    # module akka-distributed-data-experimental.
    state-store-mode = "persistence"

    # The shard saves persistent snapshots after this number of persistent
    # events. Snapshots are used to reduce recovery times.
    snapshot-after = 1000

    # Setting for the default shard allocation strategy
    least-shard-allocation-strategy {
      # Threshold of how large the difference between most and least number of
      # allocated shards must be to begin the rebalancing.
      rebalance-threshold = 10

      # The number of ongoing rebalancing processes is limited to this number.
      max-simultaneous-rebalance = 3
    }

    # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
    # works only for state-store-mode = "ddata"
    waiting-for-state-timeout = 5 s

    # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
    # works only for state-store-mode = "ddata"
    updating-state-timeout = 5 s

    # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
    # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
    # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
    # entity actors at a fix rate. The default strategy "all".
    entity-recovery-strategy = "all"

    # Default settings for the constant rate entity recovery strategy
    entity-recovery-constant-rate-strategy {
      # Sets the frequency at which a batch of entity actors is started.
      frequency = 100 ms
      # Sets the number of entity actors to be restart at a particular interval
      number-of-entities = 5
    }

    # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
    # The "role" of the singleton configuration is not used. The singleton role will
    # be the same as "akka.cluster.sharding.role".
    coordinator-singleton = ${akka.cluster.singleton}

    # The id of the dispatcher to use for ClusterSharding actors.
    # If not specified default dispatcher is used.
    # If specified you need to define the settings of the actual dispatcher.
    # This dispatcher for the entity actors is defined by the user provided
    # Props, i.e. this dispatcher is not used for the entity actors.
    use-dispatcher = ""
  }
}


include "silhouette.conf"
